# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG ZEPPELIN_DISTRIBUTION_IMAGE=apache/zeppelin:0.11.1
FROM $ZEPPELIN_DISTRIBUTION_IMAGE AS zeppelin-distribution

FROM ubuntu:22.04

LABEL maintainer="Apache Software Foundation <dev@zeppelin.apache.org>"

ARG version="0.11.1"

ENV VERSION="${version}" \
    ZEPPELIN_HOME="/opt/zeppelin"

# Install Java for zeppelin interpreter
# Install micromamba to install a python environment via conda
RUN set -ex && \
    /usr/bin/apt-get update && \
    DEBIAN_FRONTEND=noninteractive /usr/bin/apt-get install -y libssl-dev openssl openjdk-11-jre-headless wget tini bzip2 && \
    # Cleanup
    /usr/bin/apt-get clean && \
    /bin/rm -rf /var/lib/apt/lists/*

COPY bin/micromamba .
COPY bin/micromamba /usr/bin/
COPY --from=zeppelin-distribution /opt/zeppelin/bin ${ZEPPELIN_HOME}/bin
COPY log4j.properties ${ZEPPELIN_HOME}/conf/
COPY log4j_yarn_cluster.properties ${ZEPPELIN_HOME}/conf/
# Decide:
## 1) Build a huge image with all interpreters (default)
COPY --from=zeppelin-distribution /opt/zeppelin/interpreter ${ZEPPELIN_HOME}/interpreter
##RUN cp ${ZEPPELIN_HOME}/interpreter/zeppelin-interpreter*.jar /opt/zeppelin/interpreter/spark/
## 2) Build an image with only some interpreters
#### Copy interpreter-shaded JAR, needed for all interpreters
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/zeppelin-interpreter-shaded-${VERSION}.jar ${ZEPPELIN_HOME}/interpreter/zeppelin-interpreter-shaded-${VERSION}.jar
#### Copy specific interpreters,  replace "${interpreter_name}" with your interpreter. Of course you can repeat the line with defferent interpreter
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/spark
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/jdbc
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/jupyter
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/md
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/python
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/sh
#COPY --from=zeppelin-distribution /opt/zeppelin/interpreter/spark  ${ZEPPELIN_HOME}/interpreter/spark-submit

# Decide: Install conda to manage python and R packages. Maybe adjust the packages env_python_3_with_R
# Install python and R packages via conda
COPY env_python_3.yml /env_python_3.yml
# To improve the build time, the Zeppelin team recommends a conda proxy
# COPY condarc /etc/conda/condarc
RUN set -ex && \
    micromamba create -y -p /opt/conda -f env_python_3.yml && \
    micromamba clean -ay

ENV PATH=/opt/conda/bin:$PATH \
    SPARK_HOME=/opt/conda/lib/python3.9/site-packages/pyspark \
    JAVA_HOME=/usr

# Allow to modify conda packages. This allows malicious code to be injected into other interpreter sessions, therefore it is disabled by default
# chmod -R ug+rwX /opt/conda

ADD ./ojdbc8-23.2.0.0.jar ${ZEPPELIN_HOME}/lib
RUN mkdir -p "${ZEPPELIN_HOME}/logs" "${ZEPPELIN_HOME}/run" "${ZEPPELIN_HOME}/local-repo" && \
     # Allow process to edit /etc/passwd, to create a user entry for zeppelin
    chgrp root /etc/passwd && chmod ug+rw /etc/passwd && \ 
    rm /opt/zeppelin/interpreter/spark/._spark-interpreter-0.11.1.jar && rm /opt/zeppelin/interpreter/spark/scala-2.12/._spark-scala-2.12-0.11.1.jar && \
    # Give access to some specific folders
    chmod -R 775 "${ZEPPELIN_HOME}/logs" "${ZEPPELIN_HOME}/run" "${ZEPPELIN_HOME}/local-repo" "${ZEPPELIN_HOME}/lib"

ADD ./aws-java-sdk-bundle-1.12.262.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./hadoop-aws-3.3.4.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./hadoop-common-3.3.4.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./jackson-annotations-2.15.2.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./jackson-core-2.15.2.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./jackson-databind-2.15.2.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./delta-core_2.13-2.3.0.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./delta-storage-2.3.0.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./kafka-stream/* /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./commons-net-3.11.1.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./jsch-0.1.55.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./khai-thac1.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./config-1.4.2.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
ADD ./scopt_2.12-4.0.1.jar /opt/conda/lib/python3.8/site-packages/pyspark/jars/
RUN rm -f /opt/conda/lib/python3.8/site-packages/pyspark/jars/jackson-databind-2.13.4.2.jar && chmod -R 775 /opt/conda/lib/python3.8/site-packages/pyspark/jars/ && \
     pip3 install delta && python3 -m pip install cx_Oracle --upgrade && mkdir -p /opt/spark/work-dir/instantclient/lib && apt update &&  apt-get install libaio1 -y && rm -rf /var/lib/apt/lists/* && \
    apt-get autoclean && \
    apt-get clean
COPY ./instantclient_12_2/* /opt/spark/work-dir/instantclient/
RUN chmod 755 -R /opt/spark/work-dir/instantclient

RUN mkdir -p /etc/ld.so.conf.d/ && echo "/opt/spark/work-dir/instantclient" >> /etc/ld.so.conf.d/oracle-instantclient.conf && ldconfig
ENV ORACLE_HOME=/opt/spark/work-dir/instantclient

USER 1000
ENTRYPOINT [ "/usr/bin/tini", "--" ]
WORKDIR ${ZEPPELIN_HOME}
